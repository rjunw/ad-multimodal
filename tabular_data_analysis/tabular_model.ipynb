{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dacfb16f-c236-44bc-ab56-dea10cd8aed0",
   "metadata": {
    "vscode": {
     "languageId": "python"
    }
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm.notebook import tqdm\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import confusion_matrix, accuracy_score, roc_auc_score, f1_score, precision_recall_curve, roc_curve, RocCurveDisplay, PrecisionRecallDisplay\n",
    "from scipy.special import softmax\n",
    "\n",
    "from imblearn.ensemble import BalancedRandomForestClassifier\n",
    "import xgboost as xgb\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.linear_model import LogisticRegression, LogisticRegressionCV\n",
    "\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from pytorch_metric_learning import losses\n",
    "import copy\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "pd.options.display.max_rows = 999"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e9a41c9-829b-40b6-bcf3-c23a388169ec",
   "metadata": {
    "vscode": {
     "languageId": "python"
    }
   },
   "outputs": [],
   "source": [
    "dataset = \"AD_patients.csv\"\n",
    "dataset1 = \"AD_patients_subset.csv\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79180426-9da6-4f3c-ab4b-43ca6e958038",
   "metadata": {},
   "outputs": [],
   "source": [
    "patient_df = pd.read_csv(dataset, index_col=0)\n",
    "patient_df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1eb61f57-eb48-4bcb-93c2-e263dbe176bc",
   "metadata": {
    "vscode": {
     "languageId": "python"
    }
   },
   "outputs": [],
   "source": [
    "patient_df = pd.read_csv(dataset1, index_col=0)\n",
    "missingness = patient_df.isna().mean(axis=0)\n",
    "patient_df = patient_df.drop(columns=missingness[missingness > 0.3].index)\n",
    "patient_df = patient_df.apply(lambda x: x.fillna(x.mean()),axis=0)\n",
    "\n",
    "patient_df[\"family history\"] = patient_df[\"family history\"].astype(int)\n",
    "\n",
    "label = patient_df[\"AD\"].to_numpy()\n",
    "demographic_df = patient_df[patient_df.columns[1:-40]].copy()\n",
    "demographic_df[\"age\"] = 2021 - demographic_df[\"year of birth\"]\n",
    "demographic_df1 = demographic_df.drop(columns=[\"year of birth\"])\n",
    "colnames = demographic_df.columns\n",
    "demographic_df = demographic_df1.to_numpy()\n",
    "\n",
    "genotype_df = patient_df[patient_df.columns[-40:]].to_numpy()\n",
    "\n",
    "print(label.shape, demographic_df.shape, genotype_df.shape)\n",
    "\n",
    "demo_train, demo_test, geno_train, geno_test, y_train, y_test = train_test_split(\n",
    "    demographic_df, genotype_df, label, test_size=0.2, random_state=42, stratify=label)\n",
    "print(y_train.mean(), y_test.mean())\n",
    "\n",
    "demo_scaler = StandardScaler()\n",
    "demo_train = demo_scaler.fit_transform(demo_train)\n",
    "demo_test = demo_scaler.transform(demo_test)\n",
    "\n",
    "geno_scaler = StandardScaler()\n",
    "geno_train = geno_scaler.fit_transform(geno_train)\n",
    "geno_test = geno_scaler.transform(geno_test)\n",
    "\n",
    "[a.shape for a in [demo_train, demo_test, geno_train, geno_test, y_train, y_test]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17ba1cb2-8a5c-41d9-9fef-6111df2f713b",
   "metadata": {
    "vscode": {
     "languageId": "python"
    }
   },
   "outputs": [],
   "source": [
    "demo_train1, demo_val, geno_train1, geno_val, y_train1, y_val = train_test_split(\n",
    "    demo_train, geno_train, y_train, test_size=0.125, random_state=42, stratify=y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "498a2220-412b-4802-b919-0d33556a6803",
   "metadata": {
    "vscode": {
     "languageId": "python"
    }
   },
   "outputs": [],
   "source": [
    "comb_train = np.concatenate([demo_train, geno_train], axis=1)\n",
    "comb_test = np.concatenate([demo_test, geno_test], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a3664166-abe3-4d7d-bcc3-e587ba45c329",
   "metadata": {
    "vscode": {
     "languageId": "python"
    }
   },
   "outputs": [],
   "source": [
    "def get_performance(y_test, y_pred, plot=False):\n",
    "    acc = accuracy_score(y_test, y_pred.argmax(axis=1))\n",
    "    auc = roc_auc_score(y_test, y_pred[:,1])\n",
    "    f1 = f1_score(y_test, y_pred.argmax(axis=1))\n",
    "    class_acc = confusion_matrix(y_test, y_pred.argmax(axis=1), normalize=\"true\").diagonal()\n",
    "    print(f\"Accuracy: {acc}, AUC: {auc}, f1:{f1}, non-AD accuracy: {class_acc[0]}, AD accuracy: {class_acc[1]}\")\n",
    "    if plot:\n",
    "        fpr, tpr, _ = roc_curve(y_test, y_pred[:,1])\n",
    "        roc_display = RocCurveDisplay(fpr=fpr, tpr=tpr)\n",
    "        prec, recall, _ = precision_recall_curve(y_test, y_pred[:,1])\n",
    "        pr_display = PrecisionRecallDisplay(precision=prec, recall=recall)\n",
    "        fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(10, 5))\n",
    "        roc_display.plot(ax=ax1)\n",
    "        pr_display.plot(ax=ax2)\n",
    "        plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a1e8eeb-4785-4f65-aabe-106b9840fe11",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Baseline Majority class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0381546-d251-4ad9-b532-2616c5d54759",
   "metadata": {
    "vscode": {
     "languageId": "python"
    }
   },
   "outputs": [],
   "source": [
    "maj_class = np.stack([np.ones(len(y_test)), np.zeros(len(y_test))])\n",
    "get_performance(y_test, np.stack([np.ones(len(y_test)), np.zeros(len(y_test))], axis=1))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a2104bd-4c61-428b-a3b5-3620eba0b924",
   "metadata": {},
   "source": [
    "## Logistic regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f51e58a3-b668-4bbb-987a-987b74bfbaad",
   "metadata": {
    "vscode": {
     "languageId": "python"
    }
   },
   "outputs": [],
   "source": [
    "demo_lr = LogisticRegressionCV(max_iter=3000, class_weight=\"balanced\",  penalty=\"elasticnet\", solver=\"saga\", l1_ratios=[0.5, 0.8])\n",
    "demo_lr.fit(demo_train, y_train)\n",
    "y_pred = demo_lr.predict_proba(demo_test)\n",
    "demo_logreg = y_pred\n",
    "get_performance(y_test, y_pred, True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9542963b-929e-450a-8edc-0801adc3c8a1",
   "metadata": {
    "vscode": {
     "languageId": "python"
    }
   },
   "outputs": [],
   "source": [
    "coefs = demo_lr.coef_.flatten()\n",
    "weights = pd.Series(coefs, index=colnames)\n",
    "plt.figure(figsize=(6,10))\n",
    "weights.plot.barh()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b767b7cd-24c9-4789-9743-e83334ff2974",
   "metadata": {
    "vscode": {
     "languageId": "python"
    }
   },
   "outputs": [],
   "source": [
    "comb_lr = LogisticRegressionCV(max_iter=3000, class_weight=\"balanced\",  penalty=\"elasticnet\", solver=\"saga\", l1_ratios=[0.5, 0.8])\n",
    "comb_lr.fit(comb_train, y_train)\n",
    "y_pred = comb_lr.predict_proba(comb_test)\n",
    "logreg_pred = y_pred\n",
    "get_performance(y_test, y_pred, True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a848e71-8821-4fe5-9905-1889e8de76d5",
   "metadata": {
    "vscode": {
     "languageId": "python"
    }
   },
   "outputs": [],
   "source": [
    "coefs = comb_lr.coef_.flatten()\n",
    "weights = pd.Series(coefs, index=patient_df.columns[1:])\n",
    "plt.figure(figsize=(12,8))\n",
    "weights.plot.barh()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7473c3ad-0045-447d-ad55-316802a6e793",
   "metadata": {},
   "outputs": [],
   "source": [
    "demo_lr = LogisticRegressionCV(max_iter=3000, class_weight=\"balanced\",  penalty=\"l1\", solver=\"saga\")\n",
    "demo_lr.fit(demo_train, y_train)\n",
    "y_pred = demo_lr.predict_proba(demo_test)\n",
    "get_performance(y_test, y_pred, True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c1a1490f-9ab2-4ab3-9310-9568e8637149",
   "metadata": {},
   "outputs": [],
   "source": [
    "coefs = demo_lr.coef_.flatten()\n",
    "weights = pd.Series(coefs, index=colnames)\n",
    "plt.figure(figsize=(12,8))\n",
    "weights.plot.barh()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a13a6ece-a6b2-4dd3-ad12-06e85f9e658e",
   "metadata": {
    "vscode": {
     "languageId": "python"
    }
   },
   "outputs": [],
   "source": [
    "comb_lr = LogisticRegression(max_iter=1000, class_weight=\"balanced\")\n",
    "comb_lr.fit(comb_train, y_train)\n",
    "y_pred = comb_lr.predict_proba(comb_test)\n",
    "get_performance(y_test, y_pred, True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "73c2ff37-48bf-4704-8b7f-a05695116ea4",
   "metadata": {
    "vscode": {
     "languageId": "python"
    }
   },
   "outputs": [],
   "source": [
    "geno_lr = LogisticRegression(max_iter=1000, class_weight=\"balanced\")\n",
    "geno_lr.fit(geno_train, y_train)\n",
    "y_pred = geno_lr.predict_proba(geno_test)\n",
    "get_performance(y_test, y_pred, True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb46bd07-6274-4d79-9f90-68448be99c66",
   "metadata": {},
   "source": [
    "## Random Forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cbb55bf1-5e0e-4c50-bb1c-e817a8b6e7f0",
   "metadata": {
    "vscode": {
     "languageId": "python"
    }
   },
   "outputs": [],
   "source": [
    "demo_lr = BalancedRandomForestClassifier(class_weight=\"balanced_subsample\", n_estimators=500)\n",
    "demo_lr.fit(demo_train, y_train)\n",
    "y_pred = demo_lr.predict_proba(demo_test)\n",
    "randomforest_pred_demo = y_pred\n",
    "get_performance(y_test, y_pred, True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b7fb1bd-c6b3-434d-a7b1-4e2545725a83",
   "metadata": {
    "vscode": {
     "languageId": "python"
    }
   },
   "outputs": [],
   "source": [
    "importances = demo_lr.feature_importances_\n",
    "std = np.std([demo_lr.feature_importances_ for tree in demo_lr.estimators_], axis=0)\n",
    "forest_importances = pd.Series(importances, index=colnames)\n",
    "plt.figure(figsize=(12,8))\n",
    "forest_importances.plot.barh(yerr=std)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1e8119a-f6a4-4994-869f-3f8f9c0b9000",
   "metadata": {
    "vscode": {
     "languageId": "python"
    }
   },
   "outputs": [],
   "source": [
    "comb_lr = BalancedRandomForestClassifier(class_weight=\"balanced_subsample\", n_estimators=500)\n",
    "comb_lr.fit(comb_train, y_train)\n",
    "y_pred = comb_lr.predict_proba(comb_test)\n",
    "randomforest_pred_comb = y_pred\n",
    "get_performance(y_test, y_pred, True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a7d9f2d1-bb63-4a22-9794-e2e3b03833ea",
   "metadata": {},
   "source": [
    "## XGBoost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c84359b2-9333-47d6-a9f2-ec374a074366",
   "metadata": {
    "vscode": {
     "languageId": "python"
    }
   },
   "outputs": [],
   "source": [
    "demo_lr = xgb.XGBClassifier(n_estimators=200, scale_pos_weight=1000, objective=\"binary:logistic\")\n",
    "demo_lr.fit(demo_train, y_train)\n",
    "y_pred = demo_lr.predict_proba(demo_test)\n",
    "xgb_pred_demo = y_pred\n",
    "get_performance(y_test, y_pred, True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7696f4a5-f2c4-40bc-b677-b83aa53a1281",
   "metadata": {
    "vscode": {
     "languageId": "python"
    }
   },
   "outputs": [],
   "source": [
    "comb_lr = xgb.XGBClassifier(scale_pos_weight=(y_train==0).sum() / y_train.sum()*10, objective=\"binary:logistic\")\n",
    "comb_lr.fit(comb_train, y_train)\n",
    "y_pred = comb_lr.predict_proba(comb_test)\n",
    "xgb_pred_comb = y_pred\n",
    "get_performance(y_test, y_pred, True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a2b6782-d59f-4ad1-bba1-496b09a47991",
   "metadata": {
    "vscode": {
     "languageId": "python"
    }
   },
   "outputs": [],
   "source": [
    "geno_lr = xgb.XGBClassifier(scale_pos_weight=(y_train==0).sum() / y_train.sum()*10, objective=\"binary:logistic\")\n",
    "geno_lr.fit(geno_train, y_train)\n",
    "y_pred = geno_lr.predict_proba(geno_test)\n",
    "get_performance(y_test, y_pred, True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2319802b-3038-429b-ba06-78f08a060c1c",
   "metadata": {
    "vscode": {
     "languageId": "python"
    }
   },
   "outputs": [],
   "source": [
    "class ADDataset(Dataset):\n",
    "    def __init__(self, data_x, data_y):\n",
    "        self.data_x = torch.from_numpy(data_x)\n",
    "        label = np.eye(2)[data_y.astype(int)]\n",
    "        self.data_y = torch.from_numpy(label)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data_x)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        x = self.data_x[idx]\n",
    "        y = self.data_y[idx]\n",
    "        return x.to(device, dtype=torch.float32), y.to(device, dtype=torch.int64)\n",
    "\n",
    "class ADModel(nn.Module):\n",
    "    def __init__(self, input_size, hidden_1, hidden_2, hidden_3):\n",
    "        super().__init__()\n",
    "        self.linear_relu_stack = nn.Sequential(\n",
    "            nn.Linear(input_size, hidden_1),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(hidden_1, hidden_2),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(hidden_2, hidden_3),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(hidden_3, 2),\n",
    "            nn.Softmax(dim=1)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.linear_relu_stack(x)\n",
    "\n",
    "\n",
    "def categorical_accuracy(preds, y):\n",
    "    \"\"\"\n",
    "    Returns accuracy per batch, i.e. if you get 8/10 right, this returns 0.8, NOT 8\n",
    "    \"\"\"\n",
    "    max_preds = preds.argmax(dim = 1, keepdim = True) # get the index of the max probability\n",
    "    y = y.argmax(dim=1)\n",
    "    correct = max_preds.squeeze(1).eq(y)\n",
    "    return correct.sum().cpu() / torch.FloatTensor([y.shape[0]])\n",
    "\n",
    "def train(model, iterator, optimizer, criterion):\n",
    "    epoch_loss = 0\n",
    "    epoch_acc = 0\n",
    "    model.train()\n",
    "    for batchID, (input, target) in enumerate(iterator):\n",
    "        optimizer.zero_grad()\n",
    "        predictions = model(input).float()\n",
    "        loss = criterion(predictions, target.float())\n",
    "        acc = categorical_accuracy(predictions,target)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        epoch_loss += loss.item()\n",
    "        epoch_acc += acc.item()\n",
    "    return epoch_loss / len(iterator), epoch_acc / len(iterator)\n",
    "\n",
    "def evaluate(model, iterator, criterion):\n",
    "    preds = []\n",
    "    truth = []\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        for batchID, (input, target) in enumerate(iterator):\n",
    "            predictions = model(input.float()).cpu()\n",
    "            # target_onehot = nn.functional.one_hot(target, num_classes=2).cpu()\n",
    "            preds.append(predictions.numpy())\n",
    "            truth.append(target.cpu().numpy())\n",
    "    truth = np.concatenate(truth, axis=0)\n",
    "    preds = np.concatenate(preds, axis=0) \n",
    "    \n",
    "    return truth, preds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f06a68ff-9d0a-4ee0-807d-a410b7a3ccf8",
   "metadata": {
    "vscode": {
     "languageId": "python"
    }
   },
   "outputs": [],
   "source": [
    "demo = [demo_train1, demo_val, demo_test]\n",
    "geno = [geno_train1, geno_val, geno_test]\n",
    "comb = [\n",
    "    np.concatenate([demo_train1, geno_train1], axis=1), \n",
    "    np.concatenate([demo_val, geno_val], axis=1), \n",
    "    np.concatenate([demo_test, geno_test], axis=1)\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b40c5259-3c4a-43ff-9025-45da0b751a54",
   "metadata": {
    "vscode": {
     "languageId": "python"
    }
   },
   "outputs": [],
   "source": [
    "x_train, x_val, x_test = demo\n",
    "BATCH_SIZE = 512\n",
    "dataset_Train = ADDataset(x_train, y_train1)\n",
    "dataset_Val = ADDataset(x_val, y_val)\n",
    "dataset_Test = ADDataset(x_test, y_test)\n",
    "dataloader_train = DataLoader(dataset = dataset_Train, batch_size = BATCH_SIZE, shuffle = True)\n",
    "dataloader_val = DataLoader(dataset = dataset_Val, batch_size = BATCH_SIZE, shuffle = True)\n",
    "dataloader_test = DataLoader(dataset = dataset_Test, batch_size = BATCH_SIZE, shuffle = False)\n",
    "\n",
    "model = ADModel(x_train.shape[1], 50, 20, 10).to(device)\n",
    "weight = torch.tensor([sum(y_train1==1)/len(y_train1), sum(y_train1==0)/len(y_train1)]).to(device)\n",
    "# weight = torch.tensor([0.05, 0.95]).to(device)\n",
    "criterion = nn.CrossEntropyLoss(weight=weight) \n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n",
    "\n",
    "N_EPOCHS = 20\n",
    "best_auc = 0\n",
    "for epoch in range(N_EPOCHS):\n",
    "\n",
    "    train_loss, train_acc = train(model, dataloader_train, optimizer, criterion)\n",
    "    truth, preds = evaluate(model, dataloader_val, criterion)\n",
    "    val_auc = roc_auc_score(truth[:,1], preds[:,1])\n",
    "    print(f'Epoch {epoch+1}  Train Loss: {train_loss:.3f}  Train accuracy {train_acc:.4f}  Validation metrics:', end=\" \") \n",
    "    get_performance(truth[:,1], preds)\n",
    "    if val_auc > best_auc:\n",
    "        torch.save(model.state_dict(), 'best-model-parameters.pt')\n",
    "        best_auc = val_auc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ac92f77-b91f-4ff7-bf21-e91fd33162dd",
   "metadata": {
    "vscode": {
     "languageId": "python"
    }
   },
   "outputs": [],
   "source": [
    "model.load_state_dict(torch.load('best-model-parameters.pt'))\n",
    "truth, preds = evaluate(model, dataloader_test, criterion)\n",
    "mlp_preds_demo = preds\n",
    "get_performance(truth[:,1], preds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3a57e5f-1ccf-4de7-b2fb-ea91d61740bf",
   "metadata": {
    "vscode": {
     "languageId": "python"
    }
   },
   "outputs": [],
   "source": [
    "x_train, x_val, x_test = comb\n",
    "\n",
    "dataset_Train = ADDataset(x_train, y_train1)\n",
    "dataset_Val = ADDataset(x_val, y_val)\n",
    "dataset_Test = ADDataset(x_test, y_test)\n",
    "dataloader_train = DataLoader(dataset = dataset_Train, batch_size = BATCH_SIZE, shuffle = True)\n",
    "dataloader_val = DataLoader(dataset = dataset_Val, batch_size = BATCH_SIZE, shuffle = True)\n",
    "dataloader_test = DataLoader(dataset = dataset_Test, batch_size = BATCH_SIZE, shuffle = False)\n",
    "\n",
    "\n",
    "BATCH_SIZE = 512\n",
    "model = ADModel(x_train.shape[1], 50, 25, 10).to(device)\n",
    "weight = torch.tensor([sum(y_train1==1)/len(y_train1), sum(y_train1==0)/len(y_train1)]).to(device)\n",
    "# weight = torch.tensor([0.05, 0.95]).to(device)\n",
    "criterion = nn.CrossEntropyLoss(weight=weight) \n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.0005)\n",
    "\n",
    "N_EPOCHS = 20\n",
    "best_auc = 0\n",
    "for epoch in range(N_EPOCHS):\n",
    "\n",
    "    train_loss, train_acc = train(model, dataloader_train, optimizer, criterion)\n",
    "    truth, preds = evaluate(model, dataloader_val, criterion)\n",
    "    val_auc = roc_auc_score(truth[:,1], preds[:,1])\n",
    "    print(f'Epoch {epoch+1}  Train Loss: {train_loss:.3f}  Train accuracy {train_acc:.4f}  Validation metrics:', end=\" \") \n",
    "    get_performance(truth[:,1], preds)\n",
    "    if val_auc > best_auc:\n",
    "        torch.save(model.state_dict(), 'best-model-parameters.pt')\n",
    "        best_auc = val_auc\n",
    "        \n",
    "model.load_state_dict(torch.load('best-model-parameters.pt'))\n",
    "truth, preds = evaluate(model, dataloader_test, criterion)\n",
    "mlp_preds_comb = preds\n",
    "get_performance(truth[:,1], preds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54a7954d-b099-4640-86ee-82dec652bd7b",
   "metadata": {
    "vscode": {
     "languageId": "python"
    }
   },
   "outputs": [],
   "source": [
    "class ImageGenoDataset(Dataset):\n",
    "    def __init__(self, images, modes, targets, transform):\n",
    "        \"\"\"\n",
    "        images -- Batch size # of images\n",
    "        snps -- SNP genotype matrix (0, 1, 2); (batch_size, num_genes)\n",
    "        \"\"\"\n",
    "        self.images = images.type(torch.FloatTensor)\n",
    "        self.modes = [mode.type(torch.FloatTensor) for mode in modes]\n",
    "        self.targets = targets.type(torch.FloatTensor)\n",
    "        self.transform = transform\n",
    "\n",
    "    def __getitem__(self, index): \n",
    "        sample = self.images[index]\n",
    "        modes = [mode[index] for mode in self.modes]\n",
    "        target = self.targets[index]\n",
    "        if self.transform is not None:\n",
    "            sample = self.transform(sample)\n",
    "        return sample, modes, target\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.images)\n",
    "\n",
    "class SNPEncoder(nn.Module):\n",
    "    \"\"\"\n",
    "    MLP\n",
    "    \"\"\"\n",
    "    def __init__(self, x_dim, out_dim, fc_dims, use_bn = True):\n",
    "        \"\"\"\n",
    "            fc_dims -- A list of fully connected layers\n",
    "            p -- Dropout\n",
    "        \"\"\"\n",
    "        super(SNPEncoder, self).__init__()\n",
    "        self.x_dim = x_dim\n",
    "        self.use_bn = use_bn\n",
    "        self.enc_shape = fc_dims\n",
    "        self.enc = []\n",
    "        for i in range(len(self.enc_shape)):\n",
    "            if i == 0: # set intial data fc from x -> i\n",
    "                self.enc.append(nn.Linear(self.x_dim, self.enc_shape[i]))\n",
    "            else: # fc from i-1 -> i\n",
    "                self.enc.append(nn.Linear(self.enc_shape[i-1], self.enc_shape[i]))\n",
    "\n",
    "            if self.use_bn:\n",
    "                self.enc.append(nn.BatchNorm1d(self.enc_shape[i]))\n",
    "        self.enc = nn.ModuleList(self.enc)\n",
    "        self.out = nn.Linear(fc_dims[-1], out_dim)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # encode x's until shared layer\n",
    "        for l in self.enc:\n",
    "            x = l(x)\n",
    "            if isinstance(l, nn.BatchNorm1d):\n",
    "                x = F.relu(x) \n",
    "        z = self.out(x)\n",
    "        z = F.normalize(z, dim = 1)\n",
    "        return z\n",
    "\n",
    "class MultimodalNet(nn.Module):\n",
    "    def __init__(self, img_enc, img_kwargs, mode_encs, mode_kwargs, latent_dim = 20, p = 0.3, device = 'cpu'):\n",
    "        \"\"\"\n",
    "        Multimodal network combining image encoder with other modalities\n",
    "\n",
    "            img_enc -- Image encoder\n",
    "            mode_encs -- List of models encoding each modality\n",
    "            mode_kwargs -- List of parameter dictionaries for each modality encoder\n",
    "                mode_kwargs should not have an out_dim key, but every model should have \n",
    "                an out_dim argument\n",
    "            latent_dim -- Dimension of shared latent space\n",
    "        \"\"\"\n",
    "        super(MultimodalNet, self).__init__()\n",
    "        self.im_model = img_enc(**img_kwargs, out_dim = latent_dim).to(device)\n",
    "        self.model = []\n",
    "        for i in range(len(mode_encs)):\n",
    "            self.model.append(mode_encs[i](**mode_kwargs[i], out_dim = latent_dim).to(device))\n",
    "\n",
    "    def forward(self, im, modes):\n",
    "        \"\"\"\n",
    "            im -- image tensor (batch_size x ...)\n",
    "            modes -- list of mode batch tensors\n",
    "        \"\"\"\n",
    "        z_im = self.im_model(im)\n",
    "        z_modes = []\n",
    "        for i in range(len(modes)):\n",
    "            z_modes.append(self.model[i](modes[i]))\n",
    "        \n",
    "        return z_im, z_modes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f38d1cc8-3e0e-47d2-aa10-f030613ee6b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "LARGE_NUM = 1e9\n",
    "\n",
    "class NTXentLoss(torch.nn.Module):\n",
    "    \"\"\"\n",
    "    Cross-modal self-supervised contrastive loss\n",
    "        https://github.com/HealthML/ContIG/blob/main/models/cross_modal_loss.py\n",
    "    \"\"\"\n",
    "    def __init__(self, device, batch_size, temperature, alpha_weight):\n",
    "        \"\"\"Compute loss for model.\n",
    "        temperature: a `floating` number for temperature scaling.\n",
    "        weights: a weighting number or vector.\n",
    "        \"\"\"\n",
    "        super(NTXentLoss, self).__init__()\n",
    "        self.batch_size = batch_size\n",
    "        self.temperature = temperature\n",
    "        self.alpha_weight = alpha_weight\n",
    "        self.device = device\n",
    "        self.softmax = torch.nn.Softmax(dim=-1)\n",
    "        self.criterion = torch.nn.CrossEntropyLoss(reduction=\"sum\")\n",
    "\n",
    "    def softXEnt(self, target, logits):\n",
    "        \"\"\"\n",
    "        From the pytorch discussion Forum:\n",
    "            https://discuss.pytorch.org/t/soft-cross-entropy-loss-tf-has-it-does-pytorch-have-it/69501\n",
    "        \"\"\"\n",
    "        logprobs = torch.nn.functional.log_softmax(logits, dim=1)\n",
    "        loss = -(target * logprobs).sum() / logits.shape[0]\n",
    "        return loss\n",
    "\n",
    "    def forward(self, zis, zjs, norm=True):\n",
    "        temperature = self.temperature\n",
    "        alpha = self.alpha_weight\n",
    "\n",
    "        # Get (normalized) hidden1 and hidden2.\n",
    "        if norm:\n",
    "            zis = F.normalize(zis, p=2, dim=1)\n",
    "            zjs = F.normalize(zjs, p=2, dim=1)\n",
    "\n",
    "        hidden1, hidden2 = zis, zjs\n",
    "        batch_size = hidden1.shape[0]\n",
    "\n",
    "        hidden1_large = hidden1\n",
    "        hidden2_large = hidden2\n",
    "        labels = F.one_hot(\n",
    "            torch.arange(start=0, end=batch_size, dtype=torch.int64),\n",
    "            num_classes=batch_size,\n",
    "        ).float()\n",
    "        labels = labels.to(self.device)\n",
    "\n",
    "        # Different from Image-Image contrastive learning\n",
    "        # In the case of Image-Gen contrastive learning we do not compute the intra-modal similarity\n",
    "        # masks = F.one_hot(\n",
    "        #     torch.arange(start=0, end=batch_size, dtype=torch.int64),\n",
    "        #     num_classes=batch_size,\n",
    "        # )\n",
    "        # logits_aa = torch.matmul(hidden1, torch.transpose(hidden1_large,0, 1)) / temperature\n",
    "        # logits_aa = logits_aa - masks * LARGE_NUM\n",
    "        # logits_bb = torch.matmul(hidden2,  torch.transpose(hidden2_large,0, 1)) / temperature\n",
    "        # logits_bb = logits_bb - masks * LARGE_NUM\n",
    "\n",
    "        logits_ab = (\n",
    "            torch.matmul(hidden1, torch.transpose(hidden2_large, 0, 1)) / temperature\n",
    "        )\n",
    "        logits_ba = (\n",
    "            torch.matmul(hidden2, torch.transpose(hidden1_large, 0, 1)) / temperature\n",
    "        )\n",
    "\n",
    "        loss_a = self.softXEnt(labels, logits_ab)\n",
    "        loss_b = self.softXEnt(labels, logits_ba)\n",
    "\n",
    "        return alpha * loss_a + (1 - alpha) * loss_b + F.mse_loss(logits_ab, logits_ba) # add that the logits should be the same?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f5d2733-5be0-41bf-b22b-699bc35816a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def epoch_contrastive(model, criterion, loader, epoch, \n",
    "                       w = 1, optimizer = None, device = 'cpu'):\n",
    "    \"\"\"\n",
    "    standard contrastive epoch\n",
    "    \"\"\"\n",
    "    # print(criterion)\n",
    "    if optimizer:\n",
    "        model.train()\n",
    "        mode = 'Train'\n",
    "    else:\n",
    "        model.eval()\n",
    "        mode = 'Val'\n",
    "\n",
    "    train_loss = []\n",
    "    batches = tqdm(enumerate(loader), total=len(loader))\n",
    "    batches.set_description(\"Epoch NA: Loss (NA)\")\n",
    "\n",
    "    for batch_idx, (im, modes, y) in batches:\n",
    "        im, modes, y = im.to(device), [mode.to(device) for mode in modes], y.to(device)\n",
    "        z_im, z_modes = model(im, modes)\n",
    "        loss = 0\n",
    "        for z_mode in z_modes:\n",
    "            loss = loss + w * criterion(z_im, z_mode)#, y.to(torch.int64))\n",
    "            \n",
    "        if optimizer:\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "        train_loss.append(loss.item())\n",
    "        batches.set_description(\n",
    "            \"Epoch {:d}: {:s} Loss ({:.2e})\".format(\n",
    "                epoch, mode, loss.item()\n",
    "            )\n",
    "        )\n",
    "\n",
    "    return np.mean(train_loss)\n",
    "\n",
    "def epoch_standard(model, criterion, loader, epoch, optimizer = None, device = 'cpu'):\n",
    "    \"\"\"\n",
    "    standard epoch\n",
    "    \"\"\"\n",
    "    if optimizer:\n",
    "        model.train()\n",
    "        mode = 'Train'\n",
    "    else:\n",
    "        model.eval()\n",
    "        mode = 'Val'\n",
    "\n",
    "    train_loss = []\n",
    "    batches = tqdm(enumerate(loader), total=len(loader))\n",
    "    batches.set_description(\"Epoch NA: Loss (NA) ACC (NA)\")\n",
    "\n",
    "    count = 0\n",
    "    correct = 0\n",
    "    weight = torch.tensor([sum(y_train1==1)/len(y_train1), sum(y_train1==0)/len(y_train1)]).float().to(device)\n",
    "    for batch_idx, (x, _, y) in batches:\n",
    "        x, y = x.to(device), y.to(device)\n",
    "        z = model(x)\n",
    "        # print(z.shape, y.shape, type(z), type(y))\n",
    "        loss = F.cross_entropy(z, y.to(torch.int64), weight = weight) # criterion isnt working??\n",
    "        if optimizer:\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "        correct += (z.max(axis = 1).indices == y).float().sum()\n",
    "        count += y.shape[0]\n",
    "\n",
    "        train_loss.append(loss.item())\n",
    "        batches.set_description(\n",
    "            \"Epoch {:d}: {:s} Loss ({:.2e}) ACC ({:.2e})\".format(\n",
    "                epoch, mode, loss.item(), 100 * correct / count\n",
    "            )\n",
    "        )\n",
    "\n",
    "    return np.mean(train_loss), (100 * correct/count).detach().cpu().numpy()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5670e75-c0a8-4e4d-bf89-f23ea7e03ff4",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = ImageGenoDataset(torch.from_numpy(demo_train1), [torch.from_numpy(geno_train1)], torch.from_numpy(y_train1), transform = None)\n",
    "val_dataset = ImageGenoDataset(torch.from_numpy(demo_val), [torch.from_numpy(geno_val)], torch.from_numpy(y_val), transform = None)\n",
    "test_dataset = ImageGenoDataset(torch.from_numpy(demo_test), [torch.from_numpy(geno_test)], torch.from_numpy(y_test), transform = None)\n",
    "\n",
    "BATCH_SIZE = 128\n",
    "train_loader = DataLoader(train_dataset, batch_size = BATCH_SIZE, shuffle = True)\n",
    "val_loader = DataLoader(val_dataset, batch_size = BATCH_SIZE, shuffle = False)\n",
    "test_loader = DataLoader(test_dataset, batch_size = BATCH_SIZE, shuffle = False)\n",
    "\n",
    "for im, geno, y in train_loader:\n",
    "    print(im.shape, geno[0].shape, y.shape)\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d7c9b51b-970f-4a03-8650-670e8befa104",
   "metadata": {},
   "outputs": [],
   "source": [
    "Z_DIM = 30\n",
    "LR = 1e-3\n",
    "demo_kwargs = {\n",
    "    'x_dim' : 54, \n",
    "    'fc_dims' : [200, 50],\n",
    "    'use_bn' : True\n",
    "}\n",
    "geno_kwargs = {\n",
    "    'x_dim' : 40, \n",
    "    'fc_dims' : [200, 50],\n",
    "    'use_bn' : True\n",
    "}\n",
    "multi_model = MultimodalNet(SNPEncoder, demo_kwargs, \n",
    "                            [SNPEncoder], [geno_kwargs],\n",
    "                            latent_dim = Z_DIM,\n",
    "                            device = device).to(device)\n",
    "optimizer = optim.AdamW(multi_model.parameters(), lr = LR)\n",
    "scheduler = optim.lr_scheduler.ExponentialLR(optimizer, gamma = 0.95)\n",
    "criterion_cl = NTXentLoss(device, \n",
    "                          BATCH_SIZE, \n",
    "                          temperature = 0.1, \n",
    "                          alpha_weight = 0.5) # 0.75 weight towards images as in ContIG paper"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2e5eeee-3fc8-4e63-89d7-5225ed308778",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "# fit model\n",
    "NUM_EPOCHS = 25\n",
    "###### Train Model ######\n",
    "train_losses = []\n",
    "val_losses = []\n",
    "best_val = float(\"inf\")\n",
    "best_epoch = 0\n",
    "\n",
    "for epoch in tqdm(range(NUM_EPOCHS)):\n",
    "\n",
    "    # train \n",
    "    train_loss = epoch_contrastive(multi_model, criterion_cl, train_loader, epoch, w = 1, optimizer = optimizer, device = device)\n",
    "    train_losses.append(train_loss)\n",
    "\n",
    "    # eval \n",
    "    val_loss = epoch_contrastive(multi_model, criterion_cl, val_loader, epoch, w = 1, optimizer = None, device = device)\n",
    "    val_losses.append(val_loss)\n",
    "\n",
    "    scheduler.step()\n",
    "\n",
    "    # retain best val\n",
    "    if best_val > val_losses[-1]:\n",
    "        print(f\"Updating at epoch {epoch}\")\n",
    "        best_val = val_losses[-1]\n",
    "        best_epoch = epoch\n",
    "        # save model parameter/state dictionary\n",
    "        best_model = copy.deepcopy(multi_model.state_dict())\n",
    "\n",
    "# load best weights\n",
    "print(f\"Best epoch at {best_epoch} with {'NTXent'} loss: {best_val}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4bf7dfbd-6c54-4395-82cd-b4fef9eb26ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(list(range(NUM_EPOCHS)), train_losses, label='Training', c = 'b')\n",
    "plt.plot(list(range(NUM_EPOCHS)), val_losses, label='Validation', c = 'm')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c6ce322b-ce4a-4f41-bd3b-3ffc2f78d3e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# multi_model.load_state_dict(best_model)\n",
    "\n",
    "LR = 3e-5\n",
    "gamma = 0.95\n",
    "classifier = nn.Sequential(multi_model.im_model, \n",
    "                            ADModel(Z_DIM, 50, 20, 10)).to(device)\n",
    "weight = torch.tensor([sum(y_train1==1)/len(y_train1), sum(y_train1==0)/len(y_train1)]).to(device)\n",
    "criterion_ce = nn.CrossEntropyLoss(weight=weight)\n",
    "optimizer = optim.AdamW(classifier.parameters(), lr = LR)\n",
    "scheduler = optim.lr_scheduler.ExponentialLR(optimizer, gamma = gamma)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "717f530f-230a-419e-99bb-f8c60880edc8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# fit model\n",
    "NUM_EPOCHS = 20\n",
    "###### Train Model ######\n",
    "train_losses = []\n",
    "train_accs = []\n",
    "val_losses = []\n",
    "val_accs = []\n",
    "best_val = float(\"inf\")\n",
    "best_epoch = 0\n",
    "\n",
    "for epoch in tqdm(range(NUM_EPOCHS)):\n",
    "\n",
    "    # train \n",
    "    train_loss, train_acc = epoch_standard(classifier, criterion_ce, train_loader, epoch, optimizer = optimizer, device = device)\n",
    "    train_losses.append(train_loss)\n",
    "    train_accs.append(train_acc)\n",
    "\n",
    "    # eval \n",
    "    val_loss, val_acc = epoch_standard(classifier, criterion_ce, val_loader, epoch, optimizer = None, device = device)\n",
    "    val_losses.append(val_loss)\n",
    "    val_accs.append(val_acc)\n",
    "    scheduler.step()\n",
    "    # retain best val\n",
    "    if best_val > val_losses[-1]:\n",
    "        print(f\"Updating at epoch {epoch}\")\n",
    "        best_val = val_losses[-1]\n",
    "        best_epoch = epoch\n",
    "        # save model parameter/state dictionary\n",
    "        best_model = copy.deepcopy(classifier.state_dict())\n",
    "classifier.load_state_dict(best_model)\n",
    "# load best weights\n",
    "print(f\"Best epoch at {best_epoch} with {'CrossEntropy'} loss: {best_val}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa58eadc-596d-4234-bcba-9e7dc0ee32e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(list(range(NUM_EPOCHS)), train_losses, label='Training', c = 'b')\n",
    "plt.plot(list(range(NUM_EPOCHS)), val_losses, label='Validation', c = 'm')\n",
    "plt.title('Loss')\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n",
    "plt.plot(list(range(NUM_EPOCHS)), train_accs, label='Training', c = 'b')\n",
    "plt.plot(list(range(NUM_EPOCHS)), val_accs, label='Validation', c = 'm')\n",
    "plt.title('Accuracy')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dad94350-83a5-42f9-addc-d8ca7e7d5d5c",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = []\n",
    "y_true = []\n",
    "for im, geno, y in test_loader:\n",
    "    y = y.type(torch.int64)\n",
    "    im = im.to(device)\n",
    "    y_pred += list(classifier(im).detach().cpu().numpy())\n",
    "    y_true += list(y.detach().cpu().numpy())\n",
    "    \n",
    "get_performance(y_true, np.stack(y_pred, axis=0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a0e7f93-7fa1-4812-8716-cb70886d21a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "contrast_pred = np.stack(y_pred, axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a20b420-0808-4f34-bd2f-00a1029059a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions = [\n",
    "    np.stack([np.ones(len(y_test)), np.zeros(len(y_test))], axis=1), \n",
    "    demo_logreg, logreg_pred, \n",
    "    randomforest_pred_demo, randomforest_pred_comb, \n",
    "    xgb_pred_demo, xgb_pred_comb, \n",
    "    mlp_preds_demo, mlp_preds_comb, \n",
    "    contrast_pred\n",
    "]\n",
    "names = [\n",
    "    \"MajorityClass\", \n",
    "    \"LogisticReg (lab)\", \"LogisticReg (lab+genotype)\", \n",
    "    \"BalancedRF (lab)\", \"BalancedRF (lab+genotype)\", \n",
    "    \"XGBoost (lab)\", \"XGBoost (lab+genotype)\", \n",
    "    \"MLP (lab)\", \"MLP (lab+genotype)\",\n",
    "    \"ContIG\"\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "008e3d9b-dbb7-4f4c-ae0f-f0deb7146ba7",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(8,6))\n",
    "for i, p in enumerate(predictions):\n",
    "    print(names[i])\n",
    "    get_performance(y_true, p)\n",
    "    auc = roc_auc_score(y_true, p[:,1])\n",
    "    fpr, tpr, _ = roc_curve(y_test, p[:,1])\n",
    "    plt.plot(fpr, tpr, label=names[i] + f\" AUC: {auc:.3f}\")\n",
    "    plt.ylabel(\"True Positive Rate\")\n",
    "    plt.xlabel(\"False Positive Rate\")\n",
    "plt.legend()\n",
    "plt.savefig(\"model_performance.png\", dpi=400)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a076e545-d715-40de-af92-d724aa490472",
   "metadata": {},
   "outputs": [],
   "source": [
    "[len(p) for p in predictions]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "daf9815c-4c76-4e5e-943b-52744d427f00",
   "metadata": {},
   "outputs": [],
   "source": [
    "from statsmodels.miscmodels.ordinal_model import OrderedModel\n",
    "\n",
    "demographic_df1[\"AD\"] = label\n",
    "\n",
    "mod_log = OrderedModel(\n",
    "    demographic_df1[\"AD\"], demographic_df1[demographic_df1.columns[:-1]], distr=\"logit\"\n",
    ")\n",
    "res_log = mod_log.fit(method=\"bfgs\", disp=False)\n",
    "res_log.summary()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b6996838-c13a-41fc-993d-98236c043985",
   "metadata": {},
   "outputs": [],
   "source": [
    "demographic_df1.columns[:-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50ea87b0-573a-4d18-9a71-f05d80acf6da",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
